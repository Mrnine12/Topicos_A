{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Atividade II - Topicos A.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMMvWTEV1y2n0kpRFGcwXaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mrnine12/Topicos_A/blob/main/Atividade_II_Topicos_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzBpROjWJT8o"
      },
      "source": [
        "O conjunto de dados CIFAR-10 (Instituto Canadense de Pesquisa Avançada) é uma coleção de imagens muito utilizada para treinar algoritmos de Machine Learning.\r\n",
        "\r\n",
        "O conjunto de dados possui 60.000 imagens divididas em 10 classes diferentes. As 10 classes diferentes representam aviões, carros, pássaros, gatos, veados, cães, sapos, cavalos, navios e caminhões. Existem 6.000 imagens de cada classe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmvHGxmOu3QH"
      },
      "source": [
        "**Bibliotecas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi7XK24YDuPn"
      },
      "source": [
        "import cv2\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "from keras.datasets import cifar10\r\n",
        "from tensorflow import keras\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\r\n",
        "from tensorflow.keras import layers\r\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmfPlApSDqUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ccbd9c2-4b25-4303-e608-fa808e7e6fe3"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLXBH_w5J1rL"
      },
      "source": [
        "Basicamente, aqui será apresentada uma comparação entre uma rede neural artificial e uma rede neural convolucional, técnica mais indicada para imagens, sendo que ambas as técnicas serão aplicadas ao conjunto de dados CIFAR-10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIwb2fehFu7j"
      },
      "source": [
        "## **Artificial Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLU2uRwpRymk"
      },
      "source": [
        "# Grayscale\r\n",
        "\r\n",
        "Xtreino = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in x_train])\r\n",
        "Xteste = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in x_test])\r\n",
        "\r\n",
        "# NORMALIZAR OS VALORES DOS PIXELS\r\n",
        "\r\n",
        "Xtreino = Xtreino / 255\r\n",
        "Xteste = Xteste / 255\r\n",
        "\r\n",
        "# TRANSFORMAR NUM VETOR DE DIMENSAO 1\r\n",
        "\r\n",
        "Xtreino = Xtreino.reshape((-1, 1024))\r\n",
        "Xteste = Xteste.reshape((-1, 1024))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf9O_PgQS1tY"
      },
      "source": [
        "# ARQUITETURA DA REDE\r\n",
        "\r\n",
        "model_ann = Sequential()\r\n",
        "\r\n",
        "model_ann.add(Dense(64, activation = 'relu', input_dim = 1024))\r\n",
        "model_ann.add(Dense(64, activation = 'relu'))\r\n",
        "model_ann.add(Dense(10, activation = 'softmax'))\r\n",
        "\r\n",
        "# SETUP DA REDE\r\n",
        "\r\n",
        "model_ann.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB-12cW1TBxM",
        "outputId": "e4c11a27-026b-40d3-b9a2-a3afdd206109"
      },
      "source": [
        "model_ann.fit(Xtreino, to_categorical(y_train), epochs = 20, batch_size = 32)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.1006 - accuracy: 0.2307\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9154 - accuracy: 0.3129\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8505 - accuracy: 0.3380\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8120 - accuracy: 0.3533\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7839 - accuracy: 0.3627\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7693 - accuracy: 0.3704\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7537 - accuracy: 0.3805\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7390 - accuracy: 0.3802\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7282 - accuracy: 0.3876\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7223 - accuracy: 0.3885\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7211 - accuracy: 0.3894\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7135 - accuracy: 0.3884\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.7005 - accuracy: 0.3961\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.7011 - accuracy: 0.3960\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.6956 - accuracy: 0.3972\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.6847 - accuracy: 0.3985\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.6860 - accuracy: 0.3990\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.6734 - accuracy: 0.4079\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.6704 - accuracy: 0.4041\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 2s 2ms/step - loss: 1.6647 - accuracy: 0.4107\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7facf97a8940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JD-OL-nvCtE",
        "outputId": "84a9bb55-523e-4859-dd94-fea2025b11c4"
      },
      "source": [
        "model_ann.evaluate(Xteste, to_categorical(y_test))[1]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 1ms/step - loss: 1.7109 - accuracy: 0.3919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3919000029563904"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGJBoUBZLUKs"
      },
      "source": [
        "## **Convolutional Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGHO8VNxL2fO"
      },
      "source": [
        "# Criar categorias\r\n",
        "\r\n",
        "y_train_one_hot = to_categorical(y_train)\r\n",
        "y_test_one_hot = to_categorical(y_test)\r\n",
        "\r\n",
        "# NORMALIZAR OS VALORES DOS PIXELS\r\n",
        "\r\n",
        "x_train = x_train / 255\r\n",
        "x_test = x_test / 255"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdaTQNP2MAuy"
      },
      "source": [
        "# Modelo\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Conv2D(32, (5, 5), activation = 'relu', input_shape = (32,32,3)))\r\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\r\n",
        "model.add(Conv2D(64, (5, 5), activation = 'relu'))\r\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(1000, activation = 'relu'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(500, activation = 'relu'))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(250, activation = 'relu'))\r\n",
        "model.add(Dense(10, activation = 'softmax'))\r\n",
        "\r\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer ='adam', metrics = ['accuracy'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEwSb1aEM4r2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3486e640-6dd8-4906-dec0-d2df2f0df189"
      },
      "source": [
        "hist = model.fit(x_train, y_train_one_hot, batch_size = 256, epochs = 20, validation_split = 0.2 )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "157/157 [==============================] - 69s 438ms/step - loss: 2.0134 - accuracy: 0.2346 - val_loss: 1.4601 - val_accuracy: 0.4690\n",
            "Epoch 2/20\n",
            "157/157 [==============================] - 68s 431ms/step - loss: 1.4393 - accuracy: 0.4695 - val_loss: 1.2805 - val_accuracy: 0.5468\n",
            "Epoch 3/20\n",
            "157/157 [==============================] - 68s 432ms/step - loss: 1.2499 - accuracy: 0.5514 - val_loss: 1.1598 - val_accuracy: 0.5850\n",
            "Epoch 4/20\n",
            "157/157 [==============================] - 68s 434ms/step - loss: 1.1165 - accuracy: 0.6005 - val_loss: 1.0790 - val_accuracy: 0.6184\n",
            "Epoch 5/20\n",
            "157/157 [==============================] - 68s 434ms/step - loss: 1.0139 - accuracy: 0.6403 - val_loss: 0.9999 - val_accuracy: 0.6466\n",
            "Epoch 6/20\n",
            "157/157 [==============================] - 68s 432ms/step - loss: 0.9337 - accuracy: 0.6690 - val_loss: 0.9616 - val_accuracy: 0.6620\n",
            "Epoch 7/20\n",
            "157/157 [==============================] - 68s 433ms/step - loss: 0.8532 - accuracy: 0.6962 - val_loss: 0.9676 - val_accuracy: 0.6695\n",
            "Epoch 8/20\n",
            "157/157 [==============================] - 68s 432ms/step - loss: 0.7858 - accuracy: 0.7235 - val_loss: 0.9218 - val_accuracy: 0.6799\n",
            "Epoch 9/20\n",
            "157/157 [==============================] - 68s 433ms/step - loss: 0.7121 - accuracy: 0.7486 - val_loss: 0.9125 - val_accuracy: 0.6840\n",
            "Epoch 10/20\n",
            "157/157 [==============================] - 69s 437ms/step - loss: 0.6597 - accuracy: 0.7673 - val_loss: 0.9006 - val_accuracy: 0.6949\n",
            "Epoch 11/20\n",
            "157/157 [==============================] - 68s 430ms/step - loss: 0.6130 - accuracy: 0.7839 - val_loss: 0.8932 - val_accuracy: 0.7073\n",
            "Epoch 12/20\n",
            "157/157 [==============================] - 68s 432ms/step - loss: 0.5501 - accuracy: 0.8025 - val_loss: 0.9151 - val_accuracy: 0.6992\n",
            "Epoch 13/20\n",
            "157/157 [==============================] - 68s 435ms/step - loss: 0.4949 - accuracy: 0.8248 - val_loss: 0.9540 - val_accuracy: 0.6953\n",
            "Epoch 14/20\n",
            "157/157 [==============================] - 67s 430ms/step - loss: 0.4676 - accuracy: 0.8336 - val_loss: 0.9872 - val_accuracy: 0.6985\n",
            "Epoch 15/20\n",
            "157/157 [==============================] - 68s 432ms/step - loss: 0.4395 - accuracy: 0.8424 - val_loss: 1.0213 - val_accuracy: 0.6962\n",
            "Epoch 16/20\n",
            "157/157 [==============================] - 67s 429ms/step - loss: 0.4091 - accuracy: 0.8557 - val_loss: 0.9677 - val_accuracy: 0.7066\n",
            "Epoch 17/20\n",
            "157/157 [==============================] - 67s 430ms/step - loss: 0.3674 - accuracy: 0.8688 - val_loss: 1.0093 - val_accuracy: 0.7051\n",
            "Epoch 18/20\n",
            "157/157 [==============================] - 68s 432ms/step - loss: 0.3366 - accuracy: 0.8831 - val_loss: 1.0654 - val_accuracy: 0.6970\n",
            "Epoch 19/20\n",
            "157/157 [==============================] - 68s 432ms/step - loss: 0.3377 - accuracy: 0.8803 - val_loss: 1.0198 - val_accuracy: 0.7078\n",
            "Epoch 20/20\n",
            "157/157 [==============================] - 68s 432ms/step - loss: 0.3044 - accuracy: 0.8923 - val_loss: 1.0461 - val_accuracy: 0.7029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq3qaOLmNGiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f557d89-3b85-4f93-e005-86287fd44848"
      },
      "source": [
        "model.evaluate(x_test, y_test_one_hot)[1]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 5s 18ms/step - loss: 1.0486 - accuracy: 0.7041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7041000127792358"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1Rcz-BcazIo"
      },
      "source": [
        "Ao se comparar os dois modelos, ANN e CNN, foi possivel observar que como esperado, a rede mais propicia para se trabalhar com imagens, a CNN, apresentou resultados superiores ao visto pela ANN. \r\n",
        "\r\n",
        "Para o modelo convolucional é possivel observar uma acurácia de 70%, enquanto que a acurácia vista para o modelo de rede neural tradicional fica em torno de 39%.\r\n",
        "\r\n",
        "Em ambos os casos, apesar de aumentar o numero de epocas, a acurácia do modelo não apresentou melhoras."
      ]
    }
  ]
}